import json
import logging
from enum import Enum
from itertools import zip_longest
from typing import Any, Optional
from uuid import UUID

import aioredis
from aioredis import Redis
from sqlalchemy.ext.declarative import declarative_base

from app.core.config import settings
from app.database.postgres.repository.base import BaseRepository

Base = declarative_base()

CACHE_ONE_HOUR = 60 * 60
TABLE_PREFIX: str = "_table"


def batcher(iterable, n):
    """
    iterate a list in batches of size n
    """
    args = [iter(iterable)] * n
    return zip_longest(*args)


def entity_prefix(repository: BaseRepository) -> str:
    return repository.model.__tablename__ + TABLE_PREFIX


class RedisCache:
    redis: Optional[Redis] = None

    @classmethod
    async def initialize(cls):
        cache: RedisCache = RedisCache()
        await cache.__initialize_cache()
        return cache

    async def __initialize_cache(self):
        if not self.redis:
            logging.info("Initializing cache")
            try:
                self.redis = await aioredis.from_url(settings.REDIS_URI,
                                                     decode_responses=True,
                                                     max_connections=settings.REDIS_MAX_CONNECTION)
            except Exception as e:
                logging.warning(f"Exception occurred while initialising redis {str(e)}")
        if not self.redis:
            logging.error("Could not initialize cache")

    async def delete_cache(self, key_pattern: str):
        if not self.redis:
            await self.__initialize_cache()

        if self.redis:
            try:
                # check if wild card is not present in the key_pattern, then it means single key is being deleted
                if key_pattern.find("*") == -1:
                    await self.redis.delete(key_pattern)
                else:
                    for key_batch in batcher(self.redis.scan_iter(key_pattern), 500):
                        await self.redis.delete(*key_batch)
            except ConnectionError as e:
                self.redis = None
                logging.warning(f"Connect exception occurred while deleting key_pattern {key_pattern} {e}")
            except TimeoutError as e:
                self.redis = None
                logging.warning(f"Timeout exception occurred while deleting key_pattern {key_pattern} {e}")
            except Exception as e:
                self.redis = None
                logging.warning(f"Exception occurred while deleting key_pattern {key_pattern} {e}")

    async def set_cache(self, data: Any, key: str, expiry_in_seconds: int):
        if not self.redis:
            await self.__initialize_cache()

        if self.redis:
            try:
                await self.redis.set(key, json.dumps(data), ex=expiry_in_seconds)
            except ConnectionError as e:
                self.redis = None
                logging.warning(f"Connect exception occurred while calling setting data for key {key} {e}")
            except TimeoutError as e:
                self.redis = None
                logging.warning(f"Timeout exception occurred while calling setting data for key {key} {e}")
            except Exception as e:
                self.redis = None
                logging.warning(f"Exception occurred while calling setting data for key {key} {e}")

    async def get_cache(self, key: str):
        if not self.redis:
            await self.__initialize_cache()

        if self.redis:
            data = None
            try:
                data = await self.redis.get(key)
            except ConnectionError as e:
                self.redis = None
                logging.warning(f"Connect exception occurred while calling fetching data for key {key} {e}")
            except TimeoutError as e:
                self.redis = None
                logging.warning(f"Timeout exception occurred while calling fetching data for key {key} {e}")
            except Exception as e:
                self.redis = None
                logging.warning(f"Exception occurred while calling fetching data for key {key} {e}")

            if data:
                return json.loads(data)
            else:
                logging.info(f"Cache missed for key {key}")

    async def delete_keys(self, key_pattern: str):
        if not self.redis:
            await self.__initialize_cache()

        if self.redis:
            await self.delete_cache(key_pattern=key_pattern)

    @staticmethod
    def get_entity_cache_keys(prefix: str, keys: [], **kwargs) -> str:
        """
        Key for cache is generated by this method
        - Key creation formula used is table name followed by parameter values separated by ":"
        - For example key is to be generated in user, it'll be of the form
          "user:dfe5c762-10b2-44fd-b3f2-2c6387690da8" where "dfe5c762-10b2-44fd-b3f2-2c6387690da8" is the id
          passed in parameter
        """
        logging.debug(f"prefix is {prefix} keys is {keys} and kwargs is {kwargs}")

        if len(keys) == 0:
            logging.warning("keys length is 0")
            # return ret throw exception

        in_key: str = prefix
        for key in keys:
            if type(key) is str:
                if key in kwargs:
                    in_key = f'{in_key}:{str(kwargs[key])}'
                else:
                    logging.warning(f"Could not find {key} in kwargs {kwargs}")
                    # throw exception

        logging.debug(f"Generated keys from _get_entity_cache_key is {in_key}")
        return in_key

    @staticmethod
    def to_dict(data: Base):
        return \
            {c.name: bool(getattr(data, c.name))
            if isinstance(getattr(data, c.name), bool)
            else int(getattr(data, c.name))
            if isinstance(getattr(data, c.name), int)
            else getattr(data, c.name).value
            if isinstance(getattr(data, c.name), Enum)
            else getattr(data, c.name)
            if getattr(data, c.name) in [None]
            else str(getattr(data, c.name))
             for c in data.__table__.columns}


redis_cache: RedisCache


async def initialize_cache():
    global redis_cache
    redis_cache = await RedisCache.initialize()


class GetCachedEntity(object):
    """Strictly to be used for read only.
    It does not return an orm object, it just returns a json"""

    def __init__(self, keys: [], expiry_time: Optional[int] = None):
        self.keys = keys
        if not expiry_time:
            self.expiry_time = CACHE_ONE_HOUR
        else:
            self.expiry_time = expiry_time

    def __call__(self, fn):
        async def execute_method(*args, **kwargs):
            generated_key = None
            if len(args) > 0:
                generated_key = \
                    redis_cache.get_entity_cache_keys(entity_prefix(repository=args[0]),
                                                      keys=self.keys,
                                                      **kwargs)

                data = await redis_cache.get_cache(generated_key)
                if data:
                    logging.debug(f"Data extracted from cache for generated_key {generated_key}")
                    data = args[0].model(**data)
                    try:
                        data.id = UUID(str(data.id))
                    except (ValueError, AttributeError) as e:
                        pass
                    except Exception as e:
                        logging.warning(f"Exception for id conversion to uuid {e}")
                    return data

            to_execute: Base = await fn(*args, **kwargs)
            if to_execute and generated_key:
                # If the data to be cached is of type uuid or any other type which
                # cannot be stored in json will be converted to string and hence make sure while using the data
                # string case is handled or converted to required class ot type
                await redis_cache.set_cache(redis_cache.to_dict(data=to_execute), key=generated_key,
                                            expiry_in_seconds=self.expiry_time)

            return to_execute

        return execute_method


class SetCacheEntity(object):
    """Strictly to be used for read only. It does not return an orm object, it just returns a json"""

    def __init__(self, keys, expiry_time):
        self.keys = keys
        self.expiry_time = expiry_time

    def __call__(self, fn):
        async def execute_method(*args, **kwargs):
            to_execute: Base = await fn(*args, **kwargs)
            if to_execute:
                generated_key: str = redis_cache.get_entity_cache_keys(entity_prefix(repository=args[0]),
                                                                       keys=self.keys,
                                                                       **kwargs)
                await redis_cache.set_cache(data=redis_cache.to_dict(data=to_execute),
                                            key=generated_key, expiry_in_seconds=self.expiry_time)
            return to_execute

        return execute_method
